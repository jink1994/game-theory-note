{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game Theory Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1\n",
    "##### GT intro TCP backup\n",
    "    correct or defective solution?\n",
    "##### Self-interested agents and utility theory\n",
    "    decision-making rationality\n",
    "##### defining games\n",
    "    Two standard representations\n",
    "        Noramal form: 范式博弈；players move simultaneously\n",
    "            Finite players <N,A,μ>: players, action set, utility function(payoff)\n",
    "            The standard matrix representation: 2-player as a matrix\n",
    "            A large collective action game: revolt or not(players up to 10 million)\n",
    "        Extensive form: 展开博弈；move sequentially, represented as a tree\n",
    "            Timing: in what order do things happen?\n",
    "            Information: what do players know when they act?\n",
    "##### Examples of games\n",
    "    Prisoner's dilemma ...V...\n",
    "    Games of pure competition\n",
    "        When players have exactly opposed interests, meaning there're only two players precisely;\n",
    "           constant sum game(Special case: zero sum)\n",
    "    Coordination game(which side of the road should you drive on?)\n",
    "    Games of cooperation\n",
    "        When players hhave exacty the same interests, in which players have the same utility function\n",
    "    General games: battle of the sexes(movie B or F?)\n",
    "        combines elements of cooperation and competition\n",
    "##### Nash Equilibrium intro\n",
    "    Keynes' beauty contest game: the stylized version\n",
    "        Hold a stock and the price is rising, but you have negative feeling about the stock. How can you sell the stock at arounf its peak? (what do you think of other people's thinking)\n",
    "##### Strategic reasoning\n",
    "    Same game: each player best responds to other players is the Nash Equilibrium\n",
    "    From 1 to 100, players choose the score for a model on magazines. The one who vote for the 2?3 of the average score wins the game.\n",
    "        THeoretically, everyone should vote for 1(smallest possible score)\n",
    "        Actually, 23 voters won in the first game.\n",
    "        Then in the second game, when every participants are aware of the rules, 4 voters won.\n",
    "    SHould we expect equilibria to be played?\n",
    "    Or non-equilibria?\n",
    "    Nash equilibrium maybe too perfect(requires a complete rationality for every players) to achieve\n",
    "##### Pure strategy nash equilibrium\n",
    "##### Dominant strategies\n",
    "    Domination\n",
    "        Strategy is choosing an ction ('pure strategy')\n",
    "        Strictly dominates: >\n",
    "        Very weakly dominates: >=\n",
    "    If evry player has a dominant strategy, the game will converge to a nash equilibrium\n",
    "    If every player has a strictly dominant strategy, the NE must be unique\n",
    "##### Pareto optimality\n",
    "    Analyzing games: can games be better as an outsiders?\n",
    "    Pareto-dominates: outcome 1 at least good a outcome 2 for every player\n",
    "    Why every game must have at least one Pareto optimal? By defination, if a cell is not a pareto optimal, there must be a cell pareto-dominates this cell, making the other cell pareto optimal.\n",
    "    Fun fact about prisoner's dilemma:\n",
    "        the nash equilibrium in this game is the only non-Pareto-optimal outcome.\n",
    "##### Further reading\n",
    "    A brief introduction to the basics of game theory\n",
    "    https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1968579"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2\n",
    "##### Mixed strategies and nash equilibrium\n",
    "    Security checkpoints setting in Somalia\n",
    "    mixed strategies: confuse the opponent by playing randomly\n",
    "        a strategy is a probability distribution over actions\n",
    "        Pure strategy: only one action is played with positive probability\n",
    "        Mixed strategy: more than one action is played with positive probability\n",
    "            actions: the 'support' of the mixed strategy\n",
    "    Utility under mixed strategies\n",
    "        expected utility from decision theory\n",
    "##### Best response and nash equilibrium\n",
    "    Theorem(Nash, 1950): EVERY finite game has a (mixed strategy) Nash equilibrium.\n",
    "    Mixed strategy nash equilibrium is a probability distribution for a player over actions\n",
    "##### Computing MNE\n",
    "    It's easy to compute Nash Equilibria if we can guess the support\n",
    "        A player can form a best response strategy by making other's indifferent. In this way both players are willing to not change strategy ---> MNE\n",
    "    Interpreting mixed strategy:\n",
    "        randomize to confuse opponents\n",
    "        randomize when uncertain about the other's action\n",
    "        what might happen in repeated play?\n",
    "        describe population dynamics: consider the the sample of people with deterministic strategies\n",
    "##### Hardness beyond 2*2 games\n",
    "    Two example algorithms for finding NE:\n",
    "        LCP(linear complementarity) formulation\n",
    "            Lemke-Howson Algorithm\n",
    "        Support Enumeration Method\n",
    "    The complexity of the NE:\n",
    "        PPAD-complete level\n",
    "##### Mixed strategy nash example\n",
    "    in sports and competitive games:\n",
    "        soccer penalty kicks\n",
    "            Counter intuitive result: soccer kickers should turn more to their weak side in a NE strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week3\n",
    "##### Beyond NE\n",
    "    Iterated removal of dominated strategies\n",
    "        Consider the dominated strategy removed, example: nanny jumping out a an airplane;\n",
    "    Zero-sum game\n",
    "        It turns out that this isn't an accident. In the case of zero-sum games, these three ideas; doing as well for yourself as possible, hurting the other player as much as possible, and being in Nash Equilibrium all turn out to coincede.\n",
    "    Correlated equilibrium\n",
    "##### strictly dominated strategies & iterative removal\n",
    "    strictly dominated strategy is never a best reply, which can be removed.\n",
    "        mixed strategy can be used to define domination too\n",
    "        games that are solvable using this technique are called 'dominance solvable'\n",
    "        order of removal doesn't matter\n",
    "    Weakly dominated strategies removal\n",
    "        at least one equilibrium preserved\n",
    "        order of removal can matter\n",
    "        Not best replies garanteed\n",
    "##### Applications\n",
    "    Two pigs and a lever:\n",
    "        they learned not to play a strictly dominated strategy\n",
    "        learning, evolution, and survival of the fittest\n",
    "##### Maxmin Strategies\n",
    "    Maximizes worst-case payoff for a player himself (opposite to Minmax)\n",
    "    Maxmin value = worst-case payoff\n",
    "##### Correlated equilirbium\n",
    "    examples: battle of sexes; traffic game;\n",
    "    A traffic light: a fair randomizing device\n",
    "    Correlated equilibrium: action recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4\n",
    "##### Perfect information extensive form\n",
    "    Cortes and burning of the boats 美版破釜沉舟\n",
    "    Ulysses and the Sirens\n",
    "##### Formalizing perfect information extensive form games\n",
    "    Normal form games doesn't incorporate any notion of sequence, or time, of the actions of the players\n",
    "    Extensive form:\n",
    "        perfect information extensive-form games\n",
    "            tuple(N,A,H,Z,...。)\n",
    "            Choice nodes: H---》non-terminal choice nodes\n",
    "            Action function: assigns actions to choice nodes\n",
    "            Player function: \n",
    "            Terminal nodes: Z\n",
    "            Successor function:\n",
    "            Utility function: utility value on terminal nodes\n",
    "        imperfect-information extensive games\n",
    "##### Perfect information extensive form: strategies, BR, NE\n",
    "    pure strategy is a specific specification of mapping from nodes to actions\n",
    "    From extensive form tree we can have induced normal form\n",
    "        lack of impactness\n",
    "    Theorem: every perfect information game in extensive form has a PSNE(pure strategy nash equilibrium)\n",
    "##### Subgame\n",
    "    subgame perfect nash equilibrium\n",
    "        equilibria can be subgame perfect when every subgame is a NE too\n",
    "##### Backward induction\n",
    "    computing subgame perfect equilibria\n",
    "        identify the equilibria in the bottom-most trees, and adopt these as one moves up the tree\n",
    "    Centipede game\n",
    "        What to do when theoretically an incident of zero probability happened?\n",
    "##### Subgame perfect application: Ultimatum bargaining\n",
    "    rejections violate rationality?\n",
    "    utility function could be different for different participants\n",
    "##### Imperfect information extensive form: poker!!! \n",
    "    A critical aspect of poker is the sequential play in betting/folding/calling\n",
    "    Also about rationality and motivation, which leads to different utility function\n",
    "    Characteristics:\n",
    "        Large sizes of betting strategies and possible hands(hard to draw a tree)\n",
    "        I think Libratus solved it by pruning the tree to a small size and play subgames有待确定\n",
    "##### Definition, strategies\n",
    "    Each player's choice nodes partitioned into information sets\n",
    "    Agents cannot distinguish between choice nodes in the same information sets\n",
    "    Formal definition:\n",
    "    \n",
    "<img style=\"float: left\" src=\"./GT/WeChat Screenshot_20201020080407.png\" width=300 height=300> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transition\n",
    "    NF --> IIEF and IIEF --> NF\n",
    "    When we apply each mapping in turn, the result won't be the same(game).\n",
    "    But we will still have the same strategy space and equilibria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mixed and behavioral strategies\n",
    "    Mixed strategy: randomize over pure strategies\n",
    "        \n",
    "    Behavioral strategy: independent coin toss every time an information set is encountered\n",
    "        \n",
    "    Games of imperfect recall\n",
    "        无法知道节点的上下文（如果action space一样）\n",
    "    Get p by maximizing the expected utility\n",
    "    Game: rainbow warship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Beyond subgame perfection\n",
    "    For complete information games, subgame perfection and bacward induction work\n",
    "    FOr incomplete info games, there're no proper subgames(players don't know their positions)\n",
    "    The hidden info might the strength of a new company, which will change the competing result\n",
    "    \n",
    "    How to solve incomplete info EF games:\n",
    "        Sequential equilibrium\n",
    "        Perfect bayesian equilibrium\n",
    "        THey specify players' beliefs, which are not contradicted by the actual play of the game(on the equilibrium path???)\n",
    "        Players best respond to their beliefs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week5 - Repeated games\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### repeated games\n",
    "    Many interactions occur more than once:\n",
    "        Films in a marketplace\n",
    "        Political alliances\n",
    "        Friends(favor exchange)\n",
    "        Workers(team production)\n",
    "    Example game: OPEC(more like a repeated prisoner's dilemma)\n",
    "##### Infinitely repeated games: utility\n",
    "    take time into account (future discounted reward)\n",
    "    Now that looks like reinforcement learning finally!!!\n",
    "\n",
    "    \n",
    "<img style=\"float: left\" src=\"./GT/WeChat Screenshot_20201021063830.png\" width=50 height=50>\n",
    "    \n",
    "    There're two interpretation of this β:\n",
    "        1/ The nearer future is more important\n",
    "        2/ The game has a probability of 1-β of ending every iteration\n",
    "##### Stochastic games\n",
    "    A generalization of repeated games:\n",
    "        we might change game in the middle stage(set of games)\n",
    "        The state depends on the action and the formal single state (transition probability kick in)\n",
    "    The definition assumes strategy space is the same in all games\n",
    "    Markov Decision Process is a single-agent stochastic game\n",
    "##### Learning in repeated games\n",
    "    Two types of learning is covered:\n",
    "        Fictitious play虚构博弈 (model-based learning)\n",
    "            Granddad of learning rigime(not very efficient)\n",
    "            Keep counts of opponents actions\n",
    "            Each turn:\n",
    "                Play a best response to the assessed strategy of the opponent\n",
    "                Observe the opponent's actual play and update beliefs accordingly\n",
    "            Theorem: If the empirical distribution of each player's strategies converges in fictitious play, then it converges to a Nash equilibrium (But normally it won't converge)\n",
    "                A set of sufficient conditions for the converge:\n",
    "                    1/ The game is zero sum\n",
    "                    2/ The game is solvable by iterated elimination of strictly dominated strategies\n",
    "                    3/ The game is a potential game(?)\n",
    "                    4/ THe game is 2*n and has generic payoffs(?)\n",
    "             \n",
    "        No-regret learning无悔博弈(not model-based)\n",
    "            No-regret learning rule\n",
    "            Regret matching\n",
    "                Converges to a correlated equilibrium for finite games\n",
    "    Learning in GT is different from AI(even in reinforcement learning?)\n",
    "        The environment consists of other agents, can't divorce the notion of learning from the notion of teaching\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Equilibria of infinitely repeated games\n",
    "    Strategy space\n",
    "        A pure strategy in an infinitely game is a policy(mapping from state to action)\n",
    "    Some famous strategies(repeated PD?)\n",
    "        Tit-for-tat: Cooperating ---> defect detected --->defect too--->cooperate again\n",
    "        Trigger: cooperating--->defect detected--->defect forever\n",
    "    No nash equilibria(cuz no induced normal form) when there's an infinite number of pure strategies.\n",
    "    A payoff profile is enforceable or feasible\n",
    "##### Folk Theorem\n",
    "    \n",
    "<img style=\"float: left\" src=\"./GT/WeChat Screenshot_20201021215427.png\" width=300 height=300>\n",
    "    \n",
    "    Feasibility and enforceability -----> Nash Equilibrium\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    An example for concept enforceable:\n",
    "<img style=\"float: left\" src=\"./GT/WeChat Screenshot_20201022123941.png\" width=500 height=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discounted repeated games\n",
    "    Histories of length t is a list of sets of action of all players on all time points\n",
    "    Prisoner's dilemma:\n",
    "    \n",
    "<img style=\"float: left\" src=\"./GT/WeChat Screenshot_20201022053440.png\" width=300 height=300>\n",
    "    \n",
    "    The discount factor or the probability of game continuing will change the equilibrium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A folk theorem for discounted repeated games\n",
    "    Based on key ingredients:\n",
    "        Some observation about how others behave\n",
    "        SUfficient value to the future(limit of means - extreme value) or higher discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bayesian Equilibrium\n",
    "    Explicitly models behavior in an uncertain environment\n",
    "    Players choose strategies to maximize their payoffs in response to others accounting for:\n",
    "        strategic uncertainty (expecting over actions of other players)\n",
    "        payoff uncertainty (expecting over types of other players)\n",
    "##### Analyzing bayesian games: example\n",
    "    A sheriff's dilemma\n",
    "##### Combine two utility charts of different types into one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week7 Coalitional games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Definition\n",
    "    Coalitional games are concerned with how to split the utility for individuals in a coalition\n",
    "    Fairness(Shapley value) and stability(core)\n",
    "##### Shapley value\n",
    "    Symmetry:\n",
    "        Interchangeable agents should receive same shares/payments\n",
    "    Dummy players:\n",
    "        Dummy players(those whose coalition add no value) should receive nothing\n",
    "    Additivity\n",
    "    Symmetry+Dummy players+additivity = Shapley Value (Marginal contribution)    \n",
    "<img style=\"float: left\" src=\"./GT/WeChat Screenshot_20201023141115.png\" width=300 height=300>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Core\n",
    "    A concept make agents want to form a grand coalition\n",
    "    A set of payoff vectors\n",
    "    Analogous to NE, but allows the deviation by groups of agents\n",
    "    \n",
    "    Existence and uniqueness\n",
    "        In the voting game, core is empty(no possible solutions can stop groups of parties from deviating)\n",
    "        The core is not always unique\n",
    "##### Simple games\n",
    "    0 or 1 as payoff\n",
    "    Veto player: 有一票否决权的选手\n",
    "    Theorem: The core is empty when there's no veto player; If there are any, the core consistes of all payoff vectors in which the nonveto players get 0. Veto选手必须有所有payoff，不然不稳定\n",
    "##### Convex game    \n",
    "    Example: Airport game\n",
    "        Convex game: Convexity is stronger condition than superadditivity\n",
    "            Theorem:\n",
    "                Every convex game has a nonempty core.\n",
    "                The Shapley value is in the core for every convex game.\n",
    "        \n",
    "<img style=\"float: left\" src=\"./GT/WeChat Screenshot_20201023144311.png\" width=300 height=300>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare Shapley value and Core: UN security council\n",
    "    Cooperative games model complex multilateral bargaining and coalition formation, without specifying the particulars of a normal form or extensive form\n",
    "        Core: Based on coalitional threats - each coalition must get at least what it can generate alone\n",
    "        Shapley value: Based on marginal contributions - what does each player contribute to each possible colition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
